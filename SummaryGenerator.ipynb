{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SummaryGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonObeid/InteractiveVisualizations/blob/master/SummaryGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cymK2B8krc-P",
        "colab_type": "code",
        "outputId": "48a9d9ea-d76f-4e9a-d525-6aea4b50d61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import urllib.request as urllib2\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import preprocessing\n",
        "import traceback\n",
        "from scipy.stats import linregress\n",
        "\n",
        "#access files from drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "csv_dir = root_dir + 'chartSummarization-master/data/kong/data/'\n",
        "json_dir = root_dir + 'chartSummarization-master/data/kong/specs/'\n",
        "summary_dir = root_dir + 'chartSummarization-master/data/kong/summaries/'\n",
        "#csv_dir = root_dir + 'chartSummarization-master/data/d3/data/'\n",
        "#json_dir = root_dir + 'chartSummarization-master/data/d3/specs/'\n",
        "#summary_dir = root_dir + 'chartSummarization-master/data/d3/summaries/'\n",
        "csvPaths = []\n",
        "jsonPaths = []\n",
        "\n",
        "for filename in os.listdir(csv_dir):\n",
        "  csvPaths.append(csv_dir+filename)\n",
        "for filename in os.listdir(json_dir):\n",
        "  jsonPaths.append(json_dir+filename)\n",
        "\n",
        "csvPaths.sort()\n",
        "jsonPaths.sort()\n",
        "print(csvPaths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "['/content/gdrive/My Drive/chartSummarization-master/data/kong/data/11.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/13.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/14.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/15.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/16.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/17.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/18.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/19.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/20.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/27.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/28.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/29.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/3.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/30.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/4.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/49.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/5.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/50.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/53.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/54.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/55.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/6.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/60.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/61.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/62.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/65.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/66.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/68.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/69.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/7.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/70.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/71.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/72.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/73.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/74.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/75.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/76.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/77.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/8.csv', '/content/gdrive/My Drive/chartSummarization-master/data/kong/data/9.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT3mbWtu3RP6",
        "colab_type": "code",
        "outputId": "dfc90c27-8914-4063-bef1-4314cd19e5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def createSummary(jsonPath, csvPath):\n",
        "  try:#open data\n",
        "    with open(jsonPath) as json_file:\n",
        "      loadedjson = json.load(json_file)\n",
        "    loadedcsv = pd.read_csv(csvPath)\n",
        "    xLabel = str(loadedjson['encoding']['x']['field'])\n",
        "    yLabel = str(loadedjson['encoding']['y']['field'])\n",
        "    dataY=loadedcsv[yLabel]\n",
        "    dataX=loadedcsv[xLabel]\n",
        "    size=str(len(dataX))\n",
        "    fieldY=str(yLabel)\n",
        "    fieldX=str(xLabel)\n",
        "    chartType = loadedjson['mark']\n",
        "    typeX=str(loadedjson['encoding']['x']['type'])\n",
        "    typeY=str(loadedjson['encoding']['y']['type'])\n",
        "    #find min and max\n",
        "    maxValue=str(max(dataY))\n",
        "    minValue=str(min(dataY))\n",
        "    maxValueIndex=dataY.idxmax()\n",
        "    minValueIndex=dataY.idxmin()\n",
        "    #run bar\n",
        "    summaryArray = []\n",
        "    if(chartType == \"bar\"):\n",
        "      summary1 = \"This bar chart has \" + size + \" categories for the \" + typeX + \" data of \" + xLabel + \".\"\n",
        "      summary2 = \" The highest value is found at \" + str(dataX[maxValueIndex]) + \" with a \" + yLabel + \" of \" + str(dataY[maxValueIndex]) + \".\"\n",
        "      summary3 = \" The lowest value is found at \" + str(dataX[minValueIndex]) + \" with a \" + yLabel + \" of \" + str(dataY[minValueIndex]) + \".\"\n",
        "      summaryArray.append(summary1)\n",
        "      summaryArray.append(summary2)\n",
        "      summaryArray.append(summary3)\n",
        "      trendsArray={\"maxIndex\":maxValueIndex,\"minIndex\":minValueIndex}\n",
        "      dict_object = dict(summary=summaryArray, trends=trendsArray)\n",
        "      json_name=jsonPath[65:len(jsonPath)-5]\n",
        "      file_path=summary_dir + json_name + \"_summary.json\"\n",
        "      file_object = open(file_path, 'w')\n",
        "      # Save dict data into the JSON file.\n",
        "      json.dump(dict_object, file_object)\n",
        "    #run line\n",
        "    if(chartType == \"line\"):\n",
        "      #determine local trends\n",
        "      trendArray = []\n",
        "      i = 1\n",
        "      #calculate variance between each adjacent y values\n",
        "      while i < (len(dataY)):\n",
        "        variance1 = dataY[i] - dataY[i-1]\n",
        "        if(variance1 > 0):\n",
        "          type1 = \"positive\"\n",
        "        elif(variance1 < 0):\n",
        "          type1 = \"negative\"\n",
        "        else:\n",
        "          type1 = \"neutral\"\n",
        "        trendArray.append(type1)\n",
        "        i = i + 1\n",
        "      localTrends = []\n",
        "      n = 0\n",
        "      startIndex = 0\n",
        "      trendLen = len(trendArray)\n",
        "      #iterate through the variances and check for trends\n",
        "      #creates dictionary containing the trend length, direction, start and end indices, and the linear regression of the trend\n",
        "      while(n < trendLen):\n",
        "        currentVal = trendArray[n-1]\n",
        "        nextVal = trendArray[n]\n",
        "        if(currentVal != nextVal or (currentVal == nextVal and n == (trendLen-1))):\n",
        "          if(n == (trendLen-1)):\n",
        "            endIndex = n+1\n",
        "          else:\n",
        "            endIndex = n\n",
        "          xRange = dataX.loc[startIndex:endIndex]\n",
        "          yRange = dataY.loc[startIndex:endIndex]\n",
        "          result = linregress(xRange, yRange)\n",
        "          slope = round(result[0],2)\n",
        "          #normalize slope to between 0 and 1\n",
        "          minimum = min(dataY)\n",
        "          maximum = max(dataY)\n",
        "          #called feature scaling /minmaxScaling\n",
        "          scaler = preprocessing.MinMaxScaler()\n",
        "          scaledX = scaler.fit_transform(xRange.values.reshape(-1, 1))\n",
        "          scaledY = scaler.fit_transform(yRange.values.reshape(-1, 1))\n",
        "          result2 = linregress(scaledX.reshape(1, -1), scaledY.reshape(1, -1))\n",
        "          normalizedSlope = round(result2[0],2)\n",
        "          if(abs(normalizedSlope) > 0.75):\n",
        "            magnitude = \"extremely\"\n",
        "          elif(abs(normalizedSlope) > 0.5 and abs(normalizedSlope) <= 0.75):\n",
        "            magnitude =\"strongly\"\n",
        "          elif(abs(normalizedSlope) > 0.25 and abs(normalizedSlope) <= 0.5):\n",
        "            magnitude =\"moderately\"\n",
        "          elif(abs(normalizedSlope) > 0 and abs(normalizedSlope) <= 0.25):\n",
        "            magnitude = \"slightly\"\n",
        "          intercept = round(result[1],2)\n",
        "          trend = \"y=\"+str(slope)+\"x+\"+str(intercept)\n",
        "          trendRange = {\"Length\":(endIndex-startIndex+1),\"direction\":currentVal,\"start\":startIndex,\"end\":endIndex,\"trend\":trend,\"magnitude\":magnitude}\n",
        "          localTrends.append(trendRange)\n",
        "          startIndex = n\n",
        "        n = n+1\n",
        "      #sort the trend dictionaries by length\n",
        "      sortedTrends = sorted(localTrends, key = lambda i: i['Length'], reverse=True)\n",
        "      localTrendSummary = \"\"\n",
        "      #determine the trend length which we consider to be significant (ex: if trend is longer than 1/4 of total length)\n",
        "      significanceRange = round(len(dataY)/4)\n",
        "      significantTrendCount = 0\n",
        "      significantTrendArray = []\n",
        "      #iterate through array of trend dictionaries, creating a new array with the trend dictionaries which are larger than the significance range\n",
        "      for trends in sortedTrends:\n",
        "        if (trends['Length'] > significanceRange):\n",
        "          significantTrendCount = significantTrendCount + 1\n",
        "          significantTrendArray.append(trends)\n",
        "        #generate the textual summary from the significant trend dictionary array at m\n",
        "      if (significantTrendCount > 0):\n",
        "        m = 1\n",
        "        startVal = str(dataX[(significantTrendArray[0]['start'])])\n",
        "        endVal = str(dataX[(significantTrendArray[0]['end'])])\n",
        "        direction = str(significantTrendArray[0]['direction'])\n",
        "        magnitude = str(significantTrendArray[0]['magnitude'])\n",
        "        #execute here if more than 1 significant trend\n",
        "        similarSynonyms = [\"Similarly\", \"Correspondingly\", \"Likewise\", \"Additionally\", \"Also\", \"In a similar manner\"]\n",
        "        contrarySynonyms = [\"Contrarily\", \"Differently\",\"On the other hand\", \"Conversely\", \"On the contrary\", \"Inversely\", \"But\"]\n",
        "        if (significantTrendCount > 1):\n",
        "          extraTrends = \"\"\n",
        "          localTrendSentence1 = \"This chart has \" + str(significantTrendCount) + \" significant trends.\"\n",
        "          summaryArray.append(localTrendSentence1)\n",
        "          localTrendSummary = \" The longest trend is \" + magnitude + \" \" + direction + \" which exists from \" + startVal + \" to \" + endVal +\".\"\n",
        "          summaryArray.append(localTrendSummary)\n",
        "          while (m < significantTrendCount):\n",
        "            if(direction == \"positive\"):\n",
        "              length = len(similarSynonyms)\n",
        "              random = randint(0, length-1)\n",
        "              synonym = similarSynonyms[random]\n",
        "              conjunction = synonym + \",\"\n",
        "            elif(direction == \"negative\"):\n",
        "              length = len(contrarySynonyms)\n",
        "              random = randint(0, length-1)\n",
        "              synonym = contrarySynonyms[random]\n",
        "              conjunction = synonym + \",\"\n",
        "            startVal = str(dataX[(significantTrendArray[m]['start'])])\n",
        "            endVal = str(dataX[(significantTrendArray[m]['end'])])\n",
        "            direction = str(significantTrendArray[m]['direction'])\n",
        "            magnitude = str(significantTrendArray[m]['magnitude'])\n",
        "            extraTrends = \" \" + conjunction + \" the next significant trend is \" + magnitude + \" \" + direction + \" which exists from \" + startVal + \" to \" + endVal +\".\"\n",
        "            summaryArray.append(extraTrends)\n",
        "            m = m + 1\n",
        "        #execute here if only 1 significant trend\n",
        "        else:\n",
        "          localTrendSentence1 = \"This chart has \" + str(significantTrendCount) + \" significant trend.\"\n",
        "          summaryArray.append(localTrendSentence1)\n",
        "          localTrendSummary = \" This trend is \" + magnitude + \" \"  + direction + \" which exists from \" + startVal + \" to \" + endVal +\".\"\n",
        "          summaryArray.append(localTrendSummary)\n",
        "      print(summaryArray)\n",
        "      dict_object = dict(summary=summaryArray, trends=significantTrendArray)\n",
        "      json_name=jsonPath[65:len(jsonPath)-5]\n",
        "      file_path=summary_dir + json_name + \"_summary.json\"\n",
        "      file_object = open(file_path, 'w')\n",
        "      # Save dict data into the JSON file.\n",
        "      json.dump(dict_object, file_object)\n",
        "  except Exception as ex:\n",
        "      print(ex)\n",
        "    \n",
        "#size = len(jsonPaths)\n",
        "#i = 0\n",
        "#while(i < size):\n",
        "createSummary(jsonPaths[0],csvPaths[0])\n",
        "  #i = i + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reduction operation 'argmax' not allowed for this dtype\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQFtXGmz9OPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outlierArray = []\n",
        "    sorted(dataY)\n",
        "    q1, q3 = np.percentile(dataY,[25,75])\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - (1.5 * iqr) \n",
        "    upper_bound = q3 + (1.5 * iqr)\n",
        "    for y in dataY:\n",
        "      if y > upper_bound or y < lower_bound:\n",
        "        outlierArray.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}